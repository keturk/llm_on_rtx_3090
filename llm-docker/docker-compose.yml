services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    runtime: nvidia
    ports:
      - "11434:11434"
    volumes:
      - /mnt/llm-models/ollama:/root/.ollama
      - /mnt/llm-data/logs/ollama:/var/log/ollama
    environment:
      - OLLAMA_DEBUG=1
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

networks:
  default:
    name: llm-network
