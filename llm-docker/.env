# LLM Docker Environment Configuration

# Storage Paths
MODELS_PATH=/mnt/llm-models
DATA_PATH=/mnt/llm-data

# Ollama Configuration
OLLAMA_MODELS=${MODELS_PATH}/ollama
OLLAMA_PORT=11434

# vLLM Configuration
VLLM_CACHE=${MODELS_PATH}/vllm
VLLM_PORT=8000
VLLM_API_KEY=your-vllm-api-key-here

# Text Generation Inference Configuration
TGI_CACHE=${MODELS_PATH}/tgi
TGI_PORT=8080

# GPU Configuration
CUDA_VISIBLE_DEVICES=0
GPU_MEMORY_UTILIZATION=0.90

# Logging
LOG_LEVEL=INFO
LOG_PATH=${DATA_PATH}/logs
