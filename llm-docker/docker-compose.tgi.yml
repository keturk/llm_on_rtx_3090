version: '3.8'

services:
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: tgi
    restart: unless-stopped
    ports:
      - "${TGI_PORT:-8080}:80"
    volumes:
      - ${TGI_CACHE:-/mnt/llm-models/tgi}:/data
      - ${LOG_PATH:-/mnt/llm-data/logs}/tgi:/var/log/tgi
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      - MODEL_ID=${TGI_MODEL:-meta-llama/Llama-3.2-3B-Instruct}
      - NUM_SHARD=1
      - MAX_INPUT_LENGTH=4096
      - MAX_TOTAL_TOKENS=8192
    shm_size: 1g

networks:
  default:
    name: llm-network
